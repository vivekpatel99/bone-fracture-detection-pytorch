{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bone Fracture Detection using Deep Learning\n",
    "\n",
    "This Jupyter Notebook explores the application of deep learning for bone fracture detection using a comprehensive X-ray image dataset.  The dataset is specifically designed for computer vision projects and aims to facilitate the development and evaluation of automated bone fracture detection algorithms.\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "The dataset encompasses X-ray images categorized into several classes, each representing a specific type of bone fracture within the upper extremities. These classes include:\n",
    "\n",
    "*   Elbow Positive\n",
    "*   Fingers Positive\n",
    "*   Forearm Fracture\n",
    "*   Humerus Fracture\n",
    "*   Shoulder Fracture\n",
    "*   Wrist Positive\n",
    "\n",
    "Each image is annotated with either bounding boxes or pixel-level segmentation masks, precisely indicating the location and extent of the detected fracture. These annotations are crucial for training and evaluating bone fracture detection algorithms, particularly object detection models.\n",
    "\n",
    "This dataset provides a valuable resource for researchers and developers working on automated fracture detection. Its diverse range of fracture classes enables the training of robust models capable of accurately identifying fractures in various regions of the upper extremities. The ultimate goal of this dataset is to accelerate the development of computer vision solutions for automated fracture detection, thereby contributing to advancements in medical diagnostics and improved patient care.\n",
    "\n",
    "**When using this dataset for your research, please cite it using the following DOI:** 10.13140/RG.2.2.14400.34569\n",
    "\n",
    "**You can also find the dataset on ResearchGate:** [https://www.researchgate.net/publication/382268240_Bone_Fracture_Detection_Computer_Vision_Project](https://www.researchgate.net/publication/382268240_Bone_Fracture_Detection_Computer_Vision_Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')], '2.18.0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.list_physical_devices('GPU'), tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading kaggle keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/BoneFractureYolo8/train/\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import main\n",
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg.DATASET_DIRS.TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import os\n",
    "\n",
    "if len(os.listdir(cfg.DATASET.DATASET_DIR)) == 0:\n",
    "    # Download the dataset\n",
    "    od.download(dataset_id_or_url=cfg.DATASET.BONE_FRACTURE_DETECTION_DATASET_URL,\n",
    "                data_dir=cfg.DATASET.DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = cfg.DATASET_DIRS.TRAIN_DIR\n",
    "VALIDATION_DIR = cfg.DATASET_DIRS.VALIDATION_DIR\n",
    "TEST_DIR = cfg.DATASET_DIRS.TEST_DIR\n",
    "\n",
    "TRAIN_IMAGE = f'{TRAIN_DIR}/images'\n",
    "TRAIN_LABELS = f'{TRAIN_DIR}/labels'\n",
    "\n",
    "VALID_IMAGE = f'{VALIDATION_DIR}/images'\n",
    "VALID_LABELS = f'{VALIDATION_DIR}/labels'\n",
    "\n",
    "TEST_IMAGE = f'{TEST_DIR}/images'\n",
    "TEST_LABELS = f'{TEST_DIR}/labels'\n",
    "\n",
    "IMG_SIZE = cfg.TRAIN.IMG_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Loads and preprocesses an image.\"\"\"\n",
    "    img = cv2.imread(image_path)  # Use cv2 for more image format support\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    # Convert to RGB (TensorFlow default)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize if needed\n",
    "    img = img / 255.0  # Normalize pixel values (important!)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_labels(label_path):\n",
    "    \"\"\"Loads and parses YOLOv8 labels.\"\"\"\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        return []  # Handle cases where no label file exists\n",
    "\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 5:  # check if the line has enough elements\n",
    "            class_id = int(parts[0])\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "\n",
    "            # Convert to normalized coordinates (0.0 - 1.0) if necessary.\n",
    "            # YOLO format is already normalized.\n",
    "            labels.append([class_id, x_center, y_center, width, height])\n",
    "        else:\n",
    "            print(f\"Skipping malformed line in {label_path}: {line}\")\n",
    "\n",
    "    return np.array(labels, dtype=np.float32)  # convert labels to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(image_files, label_files):\n",
    "    \"\"\"Creates a TensorFlow Dataset.\"\"\"\n",
    "\n",
    "    def generator():\n",
    "        for image_path, label_path in zip(image_files, label_files):\n",
    "            try:\n",
    "                image = load_image(image_path)\n",
    "            \n",
    "                labels = load_yolo_labels(label_path)\n",
    "\n",
    "                # Convert labels to TensorFlow tensor.\n",
    "                labels_tf = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "                yield image, labels_tf\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path} or {label_path}: {e}\")\n",
    "                continue  # Skip to the next file\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                          dtype=tf.float32),  # Image shape\n",
    "            # Labels shape (variable number of boxes)\n",
    "            tf.TensorSpec(shape=(None, 5), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "\n",
    "image_files = [str(f) for f in sorted(pathlib.Path(TRAIN_IMAGE).glob('*.jpg'))]\n",
    "label_files = [str(f)\n",
    "               for f in sorted(pathlib.Path(TRAIN_LABELS).glob('*.txt'))]\n",
    "\n",
    "\n",
    "# print(label_files)\n",
    "# Ensure that you have same number of images and labels\n",
    "if len(image_files) != len(label_files):\n",
    "    raise ValueError(\"Number of image files and label files do not match.\")\n",
    "\n",
    "\n",
    "# --- 4. Create and Preprocess Dataset ---\n",
    "dataset = create_tf_dataset(image_files, label_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 5), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# # Batching, shuffling, prefetching (Essential for training)\n",
    "BATCH_SIZE = 32\n",
    "dataset = dataset.shuffle(buffer_size=len(image_files))  # Shuffle the dataset\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(\n",
    "    buffer_size=tf.data.AUTOTUNE)  # Optimize data loading\n",
    "\n",
    "print(dataset.take(1))\n",
    "# for images, labels in dataset.take(1):  # take only one batch\n",
    "#     print(\"Image shape:\", images.shape)\n",
    "#     # print(\"Labels shape:\", labels.shape)\n",
    "#     print(\"Example labels:\", labels.numpy())  # Access the label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
