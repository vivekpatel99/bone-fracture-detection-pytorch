{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung and Colon Cancer Classification\n",
    "## About Dataset\n",
    "This dataset contains 25,000 histopathological images with 5 classes. All images are 768 x 768 pixels in size and are in jpeg file format.\n",
    "The images were generated from an original sample of HIPAA compliant and validated sources, consisting of 750 total images of lung tissue (250 benign lung tissue, 250 lung adenocarcinomas, and 250 lung squamous cell carcinomas) and 500 total images of colon tissue (250 benign colon tissue and 250 colon adenocarcinomas) and augmented to 25,000 using the Augmentor package.\n",
    "There are five classes in the dataset, each with 5,000 images, being:\n",
    "\n",
    "* Lung benign tissue\n",
    "* Lung adenocarcinoma\n",
    "* Lung squamous cell carcinoma\n",
    "* Colon adenocarcinoma\n",
    "* Colon benign tissue\n",
    "\n",
    "\n",
    "How to Cite this Dataset\n",
    "If you use in your research, please credit the author of the dataset:\n",
    "\n",
    "Original Article\n",
    "Borkowski AA, Bui MM, Thomas LB, Wilson CP, DeLand LA, Mastorides SM. Lung and Colon Cancer Histopathological Image Dataset (LC25000). arXiv:1912.12142v1 [eess.IV], 2019\n",
    "\n",
    "Relevant Links\n",
    "https://arxiv.org/abs/1912.12142v1\n",
    "https://github.com/tampapath/lung_colon_image_set\n",
    "Dataset BibTeX\n",
    "@article{,\n",
    "title= {LC25000 Lung and colon histopathological image dataset},\n",
    "keywords= {cancer,histopathology},\n",
    "author= {Andrew A. Borkowski, Marilyn M. Bui, L. Brannon Thomas, Catherine P. Wilson, Lauren A. DeLand, Stephen M. Mastorides},\n",
    "url= {https://github.com/tampapath/lung_colon_image_set}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.path.dirname(os.getcwd()),\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")\n",
    "\n",
    "if os.getenv(\"DATA_ROOT\") is None:\n",
    "    os.environ[\"DATA_ROOT\"] = f\"{root}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import hydra\n",
    "import opendatasets as od\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Register a resolver for torch dtypes\n",
    "OmegaConf.register_new_resolver(\"torch_dtype\", lambda name: getattr(torch, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/processed/train\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=\"1.2\", config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=\"train\")\n",
    "    print(cfg.paths.train_processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(root) / cfg.data.dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "if len(list(DATASET_DIR.iterdir())) == 0:\n",
    "    # Download the dataset\n",
    "    od.download(dataset_id_or_url=cfg.data.dataset_url, data_dir=str(DATASET_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'colon-adenocarcinoma',\n",
       " 1: 'colon-benign-tissue',\n",
       " 2: 'lung-adenocarcinoma',\n",
       " 3: 'lung-benign-tissue',\n",
       " 4: 'lung-squamous-cell-carcinoma'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"colon-adenocarcinoma\",\n",
    "    \"colon-benign-tissue\",\n",
    "    \"lung-adenocarcinoma\",\n",
    "    \"lung-benign-tissue\",\n",
    "    \"lung-squamous-cell-carcinoma\",\n",
    "]\n",
    "\n",
    "class_mapping = dict(zip(range(len(CLASS_NAMES)), CLASS_NAMES, strict=False))\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungColonDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_processed_dir: str,\n",
    "        valid_processed_dir: str,\n",
    "        test_processed_dir: str,\n",
    "        augmentations: Any,\n",
    "        valid_transforms: Any,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = True,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_data_dir = train_processed_dir\n",
    "        self.valid_data_dir = valid_processed_dir\n",
    "        self.test_data_dir = test_processed_dir\n",
    "        self.augmentations = hydra.utils.instantiate(augmentations) if augmentations is not None else None\n",
    "        self.valid_transforms = hydra.utils.instantiate(valid_transforms) if valid_transforms is not None else None\n",
    "        self.kwargs = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_workers\": num_workers,\n",
    "            \"pin_memory\": pin_memory,\n",
    "            \"persistent_workers\": persistent_workers,\n",
    "        }\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Set up the dataset for training and validation\n",
    "        self.train_dataset = ImageFolder(root=self.train_data_dir, transform=self.augmentations)\n",
    "        self.val_dataset = ImageFolder(root=self.valid_data_dir, transform=self.valid_transforms)\n",
    "        self.test_dataset = ImageFolder(root=self.test_data_dir, transform=self.valid_transforms)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            shuffle=True,\n",
    "            **self.kwargs,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            **self.kwargs,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            **self.kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = LungColonDataModule(\n",
    "    train_processed_dir=cfg.paths.train_processed_dir,\n",
    "    valid_processed_dir=cfg.paths.valid_processed_dir,\n",
    "    test_processed_dir=cfg.paths.test_processed_dir,\n",
    "    augmentations=hydra.utils.instantiate(cfg.datamodule.augmentations),\n",
    "    valid_transforms=hydra.utils.instantiate(cfg.datamodule.valid_transforms),\n",
    "    num_workers=cfg.datamodule.num_workers,\n",
    "    pin_memory=cfg.datamodule.pin_memory,\n",
    "    persistent_workers=cfg.datamodule.persistent_workers,\n",
    "    batch_size=cfg.datamodule.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape: tuple[int, int, int], output_dims: list[int], num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        input_dim = input_shape[0]\n",
    "        self.output_dims = output_dims\n",
    "        layers: list[nn.Module] = []\n",
    "\n",
    "        for out_dim in output_dims:\n",
    "            layers.append(nn.Conv2d(input_dim, out_dim, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            input_dim = out_dim\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.flattener = nn.Flatten()\n",
    "        # To determine the input size for the linear layer\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, *input_shape)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            self.flatten_dim = self.flattener(dummy_output).shape[1]\n",
    "        fc_hidden_dim = 512\n",
    "        self.classification_head = nn.Sequential(\n",
    "            self.flattener,\n",
    "            nn.Linear(self.flatten_dim, fc_hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(fc_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(self.conv_layers, self.classification_head)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        print(f\"Net.forward() received input x with shape: {x.shape}\")\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net.forward() received input x with shape: torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [1, 5]                    --\n",
       "├─Sequential: 1-1                        [1, 5]                    --\n",
       "│    └─Sequential: 2-1                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 32, 224, 224]         864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 32, 224, 224]         64\n",
       "│    │    └─ReLU: 3-3                    [1, 32, 224, 224]         --\n",
       "│    │    └─MaxPool2d: 3-4               [1, 32, 112, 112]         --\n",
       "│    │    └─Conv2d: 3-5                  [1, 64, 112, 112]         18,432\n",
       "│    │    └─BatchNorm2d: 3-6             [1, 64, 112, 112]         128\n",
       "│    │    └─ReLU: 3-7                    [1, 64, 112, 112]         --\n",
       "│    │    └─MaxPool2d: 3-8               [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-9                  [1, 128, 56, 56]          73,728\n",
       "│    │    └─BatchNorm2d: 3-10            [1, 128, 56, 56]          256\n",
       "│    │    └─ReLU: 3-11                   [1, 128, 56, 56]          --\n",
       "│    │    └─MaxPool2d: 3-12              [1, 128, 28, 28]          --\n",
       "│    └─Sequential: 2-2                   [1, 5]                    --\n",
       "│    │    └─Flatten: 3-13                [1, 100352]               --\n",
       "│    │    └─Linear: 3-14                 [1, 512]                  51,380,224\n",
       "│    │    └─BatchNorm1d: 3-15            [1, 512]                  1,024\n",
       "│    │    └─ReLU: 3-16                   [1, 512]                  --\n",
       "│    │    └─Linear: 3-17                 [1, 5]                    2,565\n",
       "==========================================================================================\n",
       "Total params: 51,477,285\n",
       "Trainable params: 51,477,285\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 557.16\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 44.97\n",
       "Params size (MB): 205.91\n",
       "Estimated Total Size (MB): 251.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "net = Net(\n",
    "    input_shape=(3, 224, 224),\n",
    "    output_dims=[32, 64, 128],\n",
    "    num_classes=len(CLASS_NAMES),\n",
    ")\n",
    "summary(net, (1, 3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungColonClassifier(pl.LightningModule):\n",
    "    def __init__(self, model: Any, lr: float = 1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
