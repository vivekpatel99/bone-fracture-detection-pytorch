{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung and Colon Cancer Classification\n",
    "## About Dataset\n",
    "This dataset contains 25,000 histopathological images with 5 classes. All images are 768 x 768 pixels in size and are in jpeg file format.\n",
    "The images were generated from an original sample of HIPAA compliant and validated sources, consisting of 750 total images of lung tissue (250 benign lung tissue, 250 lung adenocarcinomas, and 250 lung squamous cell carcinomas) and 500 total images of colon tissue (250 benign colon tissue and 250 colon adenocarcinomas) and augmented to 25,000 using the Augmentor package.\n",
    "There are five classes in the dataset, each with 5,000 images, being:\n",
    "\n",
    "* Lung benign tissue\n",
    "* Lung adenocarcinoma\n",
    "* Lung squamous cell carcinoma\n",
    "* Colon adenocarcinoma\n",
    "* Colon benign tissue\n",
    "\n",
    "\n",
    "How to Cite this Dataset\n",
    "If you use in your research, please credit the author of the dataset:\n",
    "\n",
    "Original Article\n",
    "Borkowski AA, Bui MM, Thomas LB, Wilson CP, DeLand LA, Mastorides SM. Lung and Colon Cancer Histopathological Image Dataset (LC25000). arXiv:1912.12142v1 [eess.IV], 2019\n",
    "\n",
    "Relevant Links\n",
    "https://arxiv.org/abs/1912.12142v1\n",
    "https://github.com/tampapath/lung_colon_image_set\n",
    "Dataset BibTeX\n",
    "@article{,\n",
    "title= {LC25000 Lung and colon histopathological image dataset},\n",
    "keywords= {cancer,histopathology},\n",
    "author= {Andrew A. Borkowski, Marilyn M. Bui, L. Brannon Thomas, Catherine P. Wilson, Lauren A. DeLand, Stephen M. Mastorides},\n",
    "url= {https://github.com/tampapath/lung_colon_image_set}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import hydra\n",
    "import lightning as pl\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import opendatasets as od\n",
    "import optuna\n",
    "import pyrootutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from hydra import compose, initialize\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from omegaconf import OmegaConf\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.path.dirname(os.getcwd()),\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")\n",
    "\n",
    "if os.getenv(\"DATA_ROOT\") is None:\n",
    "    os.environ[\"DATA_ROOT\"] = f\"{root}\"\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Register a resolver for torch dtypes\n",
    "OmegaConf.register_new_resolver(\"torch_dtype\", lambda name: getattr(torch, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/processed/train\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=\"1.2\", config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=\"train\")\n",
    "    print(cfg.paths.train_processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(root) / cfg.data.dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/processed/train'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.get(\"paths\").get(\"train_processed_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "if len(list(DATASET_DIR.iterdir())) == 0:\n",
    "    # Download the dataset\n",
    "    od.download(dataset_id_or_url=cfg.data.dataset_url, data_dir=str(DATASET_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for random number generators in pytorch, numpy and python.random\n",
    "if cfg.get(\"seed\"):\n",
    "    pl.seed_everything(cfg.seed, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"colon-adenocarcinoma\",\n",
    "    \"colon-benign-tissue\",\n",
    "    \"lung-adenocarcinoma\",\n",
    "    \"lung-benign-tissue\",\n",
    "    \"lung-squamous-cell-carcinoma\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungColonCancerDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_processed_dir: str,\n",
    "        valid_processed_dir: str,\n",
    "        test_processed_dir: str,\n",
    "        augmentations: Any,\n",
    "        valid_transforms: Any,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = True,\n",
    "        batch_size: int = 32,\n",
    "        subset_size: float | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_data_dir = train_processed_dir\n",
    "        self.valid_data_dir = valid_processed_dir\n",
    "        self.test_data_dir = test_processed_dir\n",
    "        self.augmentations = None\n",
    "        self.valid_transforms = None\n",
    "        self.subset_size = subset_size\n",
    "        if augmentations:\n",
    "            aug = hydra.utils.instantiate(augmentations)\n",
    "            self.augmentations = v2.Compose(aug)\n",
    "        if valid_transforms:\n",
    "            transforms = hydra.utils.instantiate(valid_transforms)\n",
    "            self.valid_transforms = v2.Compose(transforms)\n",
    "\n",
    "        self.kwargs = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_workers\": num_workers,\n",
    "            \"pin_memory\": pin_memory,\n",
    "            \"persistent_workers\": persistent_workers,\n",
    "        }\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def subset_indices(self, dataset, subset_size):\n",
    "        train_ds_len = len(dataset)\n",
    "        indices = np.arange(len(dataset))[: int(train_ds_len * self.subset_size)]\n",
    "        return indices\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        # Set up the dataset for training and validation\n",
    "        self.train_dataset = ImageFolder(root=self.train_data_dir, transform=self.augmentations)\n",
    "        self.val_dataset = ImageFolder(root=self.valid_data_dir, transform=self.valid_transforms)\n",
    "        self.test_dataset = ImageFolder(root=self.test_data_dir, transform=self.valid_transforms)\n",
    "\n",
    "        if self.subset_size:\n",
    "            print(f\"Using subset of size {self.subset_size} for training, validation, and testing.\")\n",
    "            # Subset the dataset\n",
    "            train_indices = self.subset_indices(self.train_dataset, self.subset_size)\n",
    "            self.train_dataset = torch.utils.data.Subset(self.train_dataset, train_indices)\n",
    "            val_indices = self.subset_indices(self.val_dataset, self.subset_size)\n",
    "            self.val_dataset = torch.utils.data.Subset(self.val_dataset, val_indices)\n",
    "            test_indices = self.subset_indices(self.test_dataset, self.subset_size)\n",
    "            self.test_dataset = torch.utils.data.Subset(self.test_dataset, test_indices)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            shuffle=True,\n",
    "            **self.kwargs,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            **self.kwargs,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            **self.kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset of size 0.1 for training, validation, and testing.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1800, 450, 250)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = LungColonCancerDataModule(\n",
    "    train_processed_dir=str(Path(root) / cfg.paths.train_processed_dir),\n",
    "    valid_processed_dir=str(Path(root) / cfg.paths.valid_processed_dir),\n",
    "    test_processed_dir=str(Path(root) / cfg.paths.test_processed_dir),\n",
    "    augmentations=cfg.datamodule.augmentations,\n",
    "    valid_transforms=cfg.datamodule.valid_transforms,\n",
    "    num_workers=cfg.datamodule.num_workers,\n",
    "    pin_memory=cfg.datamodule.pin_memory,\n",
    "    persistent_workers=cfg.datamodule.persistent_workers,\n",
    "    batch_size=cfg.datamodule.batch_size,\n",
    "    subset_size=0.1,\n",
    ")\n",
    "data_module.setup()\n",
    "# for batch in data_module.train_dataloader():\n",
    "#     x, y = batch\n",
    "#     print(x.shape, y.shape)\n",
    "#     break\n",
    "len(data_module.train_dataset), len(data_module.val_dataset), len(data_module.test_dataset)\n",
    "# (3600, 900, 500)\n",
    "# (18000, 4500, 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tuple[int, int, int],\n",
    "        conv_layers: list[int],\n",
    "        num_classes: int,\n",
    "        dropout_rate: float,\n",
    "        num_hidden_layers: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        input_dim = input_shape[0]\n",
    "        self.output_dims = conv_layers\n",
    "        layers: list[nn.Module] = []\n",
    "        # --- Convolutional layers\n",
    "        for out_dim in conv_layers:\n",
    "            layers.append(nn.Conv2d(input_dim, out_dim, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = out_dim\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        # --- To determine the input size for the linear layer\n",
    "        self.flattener = nn.Flatten()\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, *input_shape)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            self.flatten_dim = self.flattener(dummy_output).shape[1]\n",
    "        # --- Classification head\n",
    "        cls_layers = []\n",
    "        current_fc_input_features = self.flatten_dim\n",
    "        neuron_per_layer = 32\n",
    "        for _ in range(num_hidden_layers):\n",
    "            cls_layers.append(nn.Linear(current_fc_input_features, neuron_per_layer, bias=False))\n",
    "            cls_layers.append(nn.BatchNorm1d(neuron_per_layer))\n",
    "            cls_layers.append(nn.ReLU())\n",
    "            cls_layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            current_fc_input_features = neuron_per_layer\n",
    "            neuron_per_layer = neuron_per_layer * 2\n",
    "\n",
    "        cls_layers.append(nn.Linear(current_fc_input_features, num_classes))\n",
    "\n",
    "        self.classification_head = nn.Sequential(*cls_layers)\n",
    "        self.model = nn.Sequential(self.conv_layers, self.flattener, self.classification_head)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net.__init__() flatten_dim: 819200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [1, 5]                    --\n",
       "├─Sequential: 1-1                        [1, 5]                    --\n",
       "│    └─Sequential: 2-1                   [1, 32, 160, 160]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 32, 320, 320]         864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 32, 320, 320]         64\n",
       "│    │    └─ReLU: 3-3                    [1, 32, 320, 320]         --\n",
       "│    │    └─MaxPool2d: 3-4               [1, 32, 160, 160]         --\n",
       "│    │    └─Dropout: 3-5                 [1, 32, 160, 160]         --\n",
       "│    └─Flatten: 2-2                      [1, 819200]               --\n",
       "│    └─Sequential: 2-3                   [1, 5]                    --\n",
       "│    │    └─Linear: 3-6                  [1, 32]                   26,214,400\n",
       "│    │    └─BatchNorm1d: 3-7             [1, 32]                   64\n",
       "│    │    └─ReLU: 3-8                    [1, 32]                   --\n",
       "│    │    └─Dropout: 3-9                 [1, 32]                   --\n",
       "│    │    └─Linear: 3-10                 [1, 64]                   2,048\n",
       "│    │    └─BatchNorm1d: 3-11            [1, 64]                   128\n",
       "│    │    └─ReLU: 3-12                   [1, 64]                   --\n",
       "│    │    └─Dropout: 3-13                [1, 64]                   --\n",
       "│    │    └─Linear: 3-14                 [1, 128]                  8,192\n",
       "│    │    └─BatchNorm1d: 3-15            [1, 128]                  256\n",
       "│    │    └─ReLU: 3-16                   [1, 128]                  --\n",
       "│    │    └─Dropout: 3-17                [1, 128]                  --\n",
       "│    │    └─Linear: 3-18                 [1, 256]                  32,768\n",
       "│    │    └─BatchNorm1d: 3-19            [1, 256]                  512\n",
       "│    │    └─ReLU: 3-20                   [1, 256]                  --\n",
       "│    │    └─Dropout: 3-21                [1, 256]                  --\n",
       "│    │    └─Linear: 3-22                 [1, 5]                    1,285\n",
       "==========================================================================================\n",
       "Total params: 26,260,581\n",
       "Trainable params: 26,260,581\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 114.73\n",
       "==========================================================================================\n",
       "Input size (MB): 1.23\n",
       "Forward/backward pass size (MB): 52.44\n",
       "Params size (MB): 105.04\n",
       "Estimated Total Size (MB): 158.71\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (1, 3, 320, 320)\n",
    "\n",
    "net = Net(\n",
    "    input_shape=input_shape[1:],\n",
    "    conv_layers=[32],\n",
    "    dropout_rate=0.5,\n",
    "    num_classes=len(CLASS_NAMES),\n",
    "    num_hidden_layers=4,\n",
    ")\n",
    "summary(net, input_shape, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungColonClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Net,\n",
    "        optimizer: torch.optim.Optimizer | None = None,\n",
    "        # scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "        class_names: list[str] = CLASS_NAMES,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "        # self.scheduler = scheduler\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=len(class_names))\n",
    "        self.f1_score = F1Score(task=\"multiclass\", num_classes=len(class_names))\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def _common_step(self, batch, batch_idx) -> tuple[torch.Tensor, float, torch.Tensor]:\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss: Tensor = F.cross_entropy(y_hat, y)\n",
    "        score = self.accuracy(y_hat, y)\n",
    "        return loss, score, y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        loss, score, y_hat = self._common_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", score, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        loss, score, y_hat = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", score, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        loss, score, y_hat = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", score, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer  # (self.model.parameters()), self.scheduler\n",
    "\n",
    "    # return [self.optimizer], [self.scheduler(self.optimizer)]\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # Define the hyperparameters to optimize\n",
    "    total_conv_layers = trial.suggest_int(\"conv_layers\", 1, 6)\n",
    "    total_cls_hidden_layers = trial.suggest_int(\"hidden_layers\", 1, 6)\n",
    "    conv_channels = [x * 32 for x in range(1, total_conv_layers)]\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "\n",
    "    # Create the model\n",
    "    net = Net(\n",
    "        input_shape=input_shape[1:],\n",
    "        conv_layers=conv_channels,\n",
    "        dropout_rate=dropout_rate,\n",
    "        num_classes=len(CLASS_NAMES),\n",
    "        num_hidden_layers=total_cls_hidden_layers,\n",
    "    )\n",
    "    # # Create the optimizer\n",
    "    # optimizer = torch.optim.AdamW\n",
    "\n",
    "    # Create the scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    model = LungColonClassifier(model=net)\n",
    "    datamododule = LungColonCancerDataModule(\n",
    "        train_processed_dir=str(Path(root) / cfg.paths.train_processed_dir),\n",
    "        valid_processed_dir=str(Path(root) / cfg.paths.valid_processed_dir),\n",
    "        test_processed_dir=str(Path(root) / cfg.paths.test_processed_dir),\n",
    "        augmentations=cfg.datamodule.augmentations,\n",
    "        valid_transforms=cfg.datamodule.valid_transforms,\n",
    "        num_workers=cfg.datamodule.num_workers,\n",
    "        pin_memory=cfg.datamodule.pin_memory,\n",
    "        persistent_workers=cfg.datamodule.persistent_workers,\n",
    "        batch_size=cfg.datamodule.batch_size,\n",
    "        subset_size=0.1,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    # callbacks = [PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")]\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        # callbacks=callbacks,\n",
    "        max_epochs=10,\n",
    "        enable_progress_bar=True,\n",
    "        precision=32,\n",
    "        log_every_n_steps=1,\n",
    "    )\n",
    "    hyperparameters = {\"conv_layers\": conv_channels, \"hidden_layers\": total_cls_hidden_layers, \"dropout_rate\": dropout_rate}\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(model, datamodule=datamododule)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    # trainer.validate(model, datamodule=datamododule)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_acc\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=5, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
